In order to enhance the processing performance of multicore platforms, some of modern multicore processors \footnote{E.g. {Intel Core i7, AMD FX, ARM Cortex and FreeScale QorIQ} processors.} consider a shared cache level (L2-cache) besides to private caches (L1-cache). The primary goal of sharing a cache between different cores is to reduce the access requests to the main memory DRAM  %further, and by that shorten the DRAM interference time since the interference time is strongly correlated to the number of access requests
 \cite{Nowotsch14}. %On the other hand, uncontrolled sharing of cache can lead to a degradation of performance and lack of predictability where starvation can arise. 


Cache coloring policy \cite{Hyoseung13,Ye2014} is an algorithm to control the access to the shared cache level L2. It has been introduced to aid performance optimization where physical memory pages are mapped to cache pages, in contrast with old caching systems where virtual memory is mapped to the cache. This entails avoiding the clearance of cache pages on each context switch. 
During execution, the algorithm frees the old pages as necessary in order to make space for currently scheduled applications (recoloring). The coloring algorithm sorts the concurrent access requests according to their release times.  
 
%Another alternative to schedule accesses to DRAM, in presence of a three-dimensional structure (bank, row and column), is the {Hit-First} policy. The Hit-First algorithm schedules row buffer hits before misses to reduce the average memory access latency and to improve bandwidth utilization \cite{Hong99,Rixner2000}. This is due to the fact that requests hitting in the row buffer have shorter latency than a row buffer miss.

To maximize data throughput and minimize the DRAM latency, DRAM controllers in modern COTS-based systems use {First Ready-First Come First Serve} (FR-FCFS) as a DRAM policy \cite{Rixner2000,Kim14}. FR-FCFS considers a detailed DRAM architecture structured in terms of banks, rows and columns. %The DRAM scheduler can be viewed as a 2-level filter: bank level and bus level. 
The access requests can target different banks separately, where they will be queued in the corresponding bank queue with a special preference to read requests since they cause the processor to stall. % while write requests can normally be performed using write buffers. 
Access requests will be sorted at each bank queue first according to their readiness. Thereafter, the candidates selected from banks level will be further sorted at bus level where the earliest request gains access, i.e. the first request showing up at bus level among the requests being selected by bank schedulers. If no request hits the row-buffer, older requests are prioritized over younger ones.  

In our framework, we do not consider the detailed internal architecture and size of DRAM and shared cache. We focus rather on measuring the delays caused by the concurrent accesses. The reason behind this is that the impact of these characteristics on the
interference is already captured when performing the static analysis and identifying the WCRAs. Hence, estimating the optimal cache size for each application and cache recoloring are beyond the scope of this paper.
