\label{sec:introduction}


Today's embedded systems demand increasing computing power to accommodate the growing software functionality. 
Multicore platforms are finding their way into Automotive and avionic areas where they are a target to deploy safety-critical applications. In avionic systems, an ultimate goal of using multicore platforms is to leverage the computing capabilities, reduce the weight of on-board computing equipment and lower the energy consumption. Such multicore avionic systems are usually built by integrating different subsystems, potentially provided by different vendors, to enable incremental Design and Certification (iD\&C) \cite{iDC}, recommended by the standard Integrated Modular Avionics (IMA) architecture \cite{ima}. 
%
However, due to the safety guarantees and hard critical requirements performance can be sacrificed, up to a certain degradation level, if the safety and reliability are in peril. Performance is then considered as a second class property that is aimed to be as high as the system safety permits.  

In contrast to the classical federated architecture, IMA supports functions related to different subsystems to share the same computing platform with an efficient use of the hardware. Such a support is implemented using partitioning, where different partitions running on different cores compete for the access to a set of shared resources, such as shared memories and buses. 
%
%Multicore designers have achieved much progress on improvement of memory-dependent performance in caching systems and shared memories in general. 
However, having applications running simultaneously and requesting the access to the shared memories concurrently leads to interference. The non predictability resulting from interference at any shared memory level may lead to violation of the timing properties in safety-critical real-time systems. Moreover, the interference on shared memories leads cores to stall, in particular for read access requests, so that the performance is affected as well. Hence, bounding memory interference is a key factor to guarantee schedulability and improve performance \cite{Kim14}.

Strong efforts have been devoted to the analysis of safety and predictability of multicore systems \cite{Lisper14,Nowotsch14,Andersson2010,Chatto2012,5753612}, however less studies focusing on the compensation of performance while maintaining the timing predictability guarantees have been achieved \cite{DBLP:journals/corr/TeodoroKAKFS15,6280389,Sarkar:2011:PTM:1967677.1967696,10.1109/MS.2005.102,Subramanian13}. When designing a software system, there is a potential margin where a drift configuration, obtained by slightly tweaking the original system e.g. by assigning tasks differently to cores or using much optimal scheduling policies, can functionally behave in the same way while achieving better performance and satisfying the predictability requirements \cite{SHARMA2014544,Sarkar:2011:PTM:1967677.1967696,5753612}.   

This paper represents a performance-driven schedulability framework for multicore systems. Such a model-based framework  
enables to analyze and improve the performance of multicore systems when changing cores scheduling policies (design parameters), while maintaining systems schedulable so that system configurations achieving better performance can be identified and planned for deployment. We adopt the Asymmetric Multi-Processing (AMP) \cite{Huyck12} avionic scheduling architecture, where partitions are statically allocated to specific cores. Moreover, the processor scheduling alternatives we consider are memory-centric and core-centric. The platform architecture consists of a set of cores having each a local cache, and sharing the cache level L2 and DRAM. We use the cache coloring policy \cite{Hyoseung13} to arbitrate concurrent access requests to the shared cache at level 2 (L2). In addition, we adopt the policy {First Ready-First Come First Serve (FR-FCFS)} \cite{Rixner2000,Kim14} commonly used by modern COTS-based memory controllers to schedule the DRAM access requests.   
%
The application model is given by a set of periodic task sets, each of which is assigned to a given core and scheduled using either memory-centric or core-centric policies. Moreover, application tasks have explicit read and write access numbers for shared caches and DRAM. We distinguish between read and write access requests to shared memories as read actions are blocking for cores, while write actions are not blocking and can be performed using dedicated buffers.

The performance metrics we consider are the \textit{utilization of cores} and \textit{maximum delay per access request} to shared memories (L2 cache and DRAM). We provide rigorous schedulability analysis using symbolic model checking, while performance metrics are measured using statistical model checking. %Our framework is realized as an engineering toolbox to be used at design stage to identify the scheduling policy leading to higher performance while maintaining the schedulability property.


 

%This paper introduces a formal framework, using Uppaal \cite{Uppaal}, for the schedulability analysis and estimation of interference-sensitive WCET (isWCET) of multicore systems. The framework includes both local and shared caches as well as a shared DRAM. We use the cache coloring policy \cite{Hyoseung13} to arbitrate concurrent access requests to the shared cache at level 2 (L2). In addition, we adopt the policy {First Ready-First Come First Serve (FR-FCFS)} \cite{Rixner2000,Kim14} commonly used by modern COTS-based memory controllers to schedule the DRAM access requests. The schedulability analysis is rigorously performed using the symbolic model checker of Uppaal, while the isWCET is computed using the Uppaal statistical model checker. By statistical we mean  simulating different executions but not exploring the whole state space. The analysis is performed based on the WCET of individual tasks, how many times each task needs to read from and write to DRAM and L2 cache.%, as well as page coloring and FR-FCFS as scheduling policies.  



%The rest of the paper is organized as follows: Section~\ref{sec:relatedwork} reviews the related work. Section~\ref{sec:background} describes the necessary background. Section~\ref{sec:methodology} provides an overview of our work. Section~\ref{sec:modeling} presents Uppaal templates included in our framework, whereas Section~\ref{sec:analysis} describes schedulability, isWCET analysis and method for potential reallocation of tasks. %as well as migration of tasks. In Section~\ref{sec:casestudy}, a small case study is presented to show preliminary evidence of the feasibility and efficacy of the framework. Finally, Section~\ref{sec:conclusion} concludes the paper.  

